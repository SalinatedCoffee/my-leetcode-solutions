## 786. (M) K-th Smallest Prime Fraction

### `solution.py`
The most easiest method would be to simply generate all possible fractions and sort them by the actual value. Here we have opted for a min heap, which is exactly what we want since we are asked to return the `k`th *smallest* fraction. For each `i`th element of `arr`, where `i` is in the range `[0, n-1]`, we generate all possible fractions that have `arr[i]` as the numerator. For each fraction we push a tuple that contains the evaluated value of the integer(which is a float) and a list of length 2 where its values are the numerator and denominator of said fraction, in that order. Once all possible fractions have been pushed onto the heap, we pop from it `k - 1` times, after which the first item on the heap will contain the tuple that corresponds to the `k`th smallest fraction.  

#### Conclusion
The time complexity for this problem is $O(n^2\log n^2)$, where $n$ is the length of `arr`. Given a list of length $n$, there are $O(n^2)$ possible fractions. Because each and every fraction are pushed onto the heap, this step will take $O(n^2\log n^2)$ time to complete. The same can be said for the retrieval step since `k` is bound by $n^2$. The space complexity is $O(n^2)$ due to the heap.  
  


### `solution_2.py`
The previous solution can be further optimized by realizing that we can avoid having to push all possible fractions onto the heap and instead push and pop fractions at the same time.  
If the numerator of a fraction is the same, selecting the largest possible value for the denominator will yield the smallest fraction. In other words, if we select the largest value in `arr` as the denominator that fraction will be the smallest fraction among all other fractions that have the same numerator. Thus, we can generate a list of the smallest fraction for each numerator by dividing each value in `arr` with `arr[-1]`(since `arr` is already sorted in ascending order). We can also observe that given some fraction `arr[i] / arr[j]` the next smallest fraction that also has `arr[i]` as its numerator is simply `arr[i] / arr[j+1]`(if `j+1 < n`). By combining these observations with the behavior of a min heap, we can implement a faster solution based on the main idea of the previous solution.  
The previous solution generated all possible fractions and stored them in a min heap, after which `k - 1` items were popped from said heap to gain access to the `k`th smallest fraction. In this solution, we first push the smallest fraction for each numerator onto the heap and then pop items off of said heap, while pushing the next smallest fraction with the same numerator of the fraction that was just popped. Because the first item of the heap will always be the smallest stored item, this will guarantee that the fractions will be popped in ascending order of their values.  
We first populate the heap with the smallest fraction of each numerator by iterating over `arr` and dividing each value with `arr[-1]`. Then we pop an item off of the heap, after which we push the next smallest fraction with the same numerator as the fraction that was just popped off(if one exists). This is repeated `k - 1` times, after which the first item on the heap will be the `k`th smallest fraction overall.  

#### Conclusion
This solution has a time complexity of $O((n+k)\log n$, where $k$ is `k`. Initially populating the heap takes $O(n\log n)$ time, since $n$ fractions are pushed onto the heap. The following pop-push step takes $O(k\log n)$ time, since a pop-push pair of operations can happen $k$ times on a heap that can contain at most $n$ values. Hence, the overall time complexity is $O(n\log n + k\log n) = O((n+k)\log n)$. One could argue that $k$ is bound by $O(n^2)$, and thus the time complexity should be $O(n^3\log n)$, which is certainly true. However, this is a very loose bound as the maximum value of $k$ is actually $n(n-1) / 2$, and in practice will run faster than the previous solution. The space complexity is $O(n)$, due to the min heap.  
  


### `solution_3.py`
The follow up question asks us to implement a solution that runs faster than $O(n^2)$. This means that approaches based on enumerating all `k` smallest fractions are no longer viable, as `k` is bound by $n^2$. We can implement a faster solution by taking a binary search approach that uses the evaluated values of the fractions as the search space. Within that search space, we want to find the fraction that, including itself, has `k` fractions that are smaller than it. To count the number of smaller fractions we take advantage of the fact that `arr` is sorted in ascending order. If a valid fraction `arr[i] / arr[j]` is smaller than some value `x`, any fraction that has `arr[i]` as the numerator and a value in the list `arr[j:]` as the denominator is also guaranteed to be smaller than `x`. Hence, if `arr[i] / arr[j]` is the largest value smaller than `x`, we can trivially count the number of fractions that are smaller than `x` by evaluating the expression `len(arr) - j`.  
Weaving this all together, we can now start implementing the solution. The two variables `l` and `r` will be used to designate the current search space, where `l` will be the lower value and `r` the higher value. The initial search space should be set to the range `[0, 1)`. We can now start performing the binary search, first computing the middle value `m` after which we intialize the variables `max_val`, `count`, `nm`, `dm`, and `j`. `max_val` will hold the value of the largest fraction seen, which we will need later when returning the actual values. `count` will be the number of fractions smaller than `m`. `nm` and `dm` will be the index of the numerator and denominator of the fraction that evaluates to `max_val`, and `j` will be the index of the denominator currently being considered. Then, iterating over `arr`, we try using each value as the numerator and search for the denominator that would yield the largest value smaller than `m`. `count` is then updated by adding the value `n - j` to it, after which two things are checked - whether there are no more denominators left to consider, and whether the discovered fraction is larger than the previously largest fraction encountered(larger than `max_val`). For the former, we can simply exit the loop and move on to the next search space. For the latter, we update the values `max_val`, `nm`, and `dm` accordingly. Once the counting step is complete we can halve the search space based on the value of `count`. If it is equal to `k`, we have found our fraction and so we return `[arr[nm], arr[dm]`. If it is larger than `k`, we need to select the smaller half, and vice versa. The reason that we keep track of the largest fraction seen is because we do not maintain an explicit list of all fractions smaller than `m`, but rather the number of fractions. As such, we need to know what the largest fraction exactly is in order to return the correct values whenever we count exactly `k` fractions smaller than the midpoint.  
  
#### Conclusion
The time complexity of this solution is $O(n\log m)$, where $m$ is the largest value in `arr`(which is simply `arr[-1]`). Each evaluation of a search space takes $O(n)$ time since `arr` is scanned once(`j` is bound by $n$ and only increases, with the scan exiting whenever `j` reaches $n$). The halving of the search space continues until there are exactly `k` fractions that are smaller than the midpoint. This happens when the search space is smaller than the smallest possible difference between two fractions, which is $1/m^2$. Hence, the number of halvings will be $\log(m^2)$. Combining this with the time it takes to process each search space, the overall time complexity becomes $O(n\log m^2) = O(n\log m)$. The space complexity is $O(1)$.  
  
  